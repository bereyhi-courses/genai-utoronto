<!DOCTYPE html>
<html>

  <head>
  




  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title> Materials - Deep Generative Models / Summer 2025 </title>
  <meta name="description" content="Materials - Deep Generative Models / Summer 2025">
  
  <link rel="stylesheet" href="/genai-utoronto/_css/main.css">
  <link rel="canonical" href="http://localhost:4000/genai-utoronto/materials/">
  <link rel="alternate" type="application/rss+xml" title="Deep Generative Models / Summer 2025 - University of Toronto" href="http://localhost:4000/genai-utoronto/feed.xml" />
<link rel='stylesheet' id='open-sans-css'  href='//fonts.googleapis.com/css?family=Open+Sans%3A300italic%2C400italic%2C600italic%2C300%2C400%2C600&#038;subset=latin%2Clatin-ext&#038;ver=4.2.4' type='text/css' media='all' />
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:600italic,600,400,400italic' rel='stylesheet' type='text/css'>




<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">
<link rel="stylesheet" href="/genai-utoronto/assets/css/custom.css">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper" style="z-index: 100;">
      <table><tr>
          <td><img width="75" src="/genai-utoronto/_images/logo.png" valign="middle"></td>
          <td style="padding-left:10px;"><a class="schoolname" style="font-size: 15px; color: black;" class="site-title" href="https://www.ece.utoronto.ca">University of Toronto</a>
          <br/>
          <span class="site-title" style="font-size: 24px;font-weight: bold; color: black; display: block;">Deep Generative Models</span>
          <!-- <span style="color: black; margin-top: -2px;margin-bottom: -5px;" class="site-title" ><a href="/genai-utoronto/" title="Deep Generative Models / Summer 2025 - University of Toronto"><b>Deep Generative Models</a></b></span> -->
          <br/>
          <span class="coursesemeter" style="font-size: 15px;font-weight: bold;margin-top: 10px; color: black; display: block;">Summer 2025</span>
          </td>
        </tr></table>

    <nav class="site-nav">

      <a href="#" class="menu-icon menu.open">
        <svg viewBox="0 0 18 15">
          <path fill="#000000" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#000000" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#000000" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>  

    <div class="trigger"><h1>Main Navigation</h1>

 <ul class="menu">
    
    <li>
        <a class="page-link" href="/genai-utoronto/">
            <i class="fa fa-home fa-lg"></i> Home
        </a>
    </li>
    
    <li>
        <a class="page-link" href="/genai-utoronto/schedule/">
            <i class="fas fa-calendar-alt"></i> Schedule
        </a>
    </li>
    
    <li>
        <a class="page-link" href="/genai-utoronto/lectures/">
            <i class="fas fa-video"></i> Lecture Videos
        </a>
    </li>
    
    <li>
        <a class="page-link" href="/genai-utoronto/materials/">
            <i class="fas fa-book-reader"></i> Materials
        </a>
    </li>
    
    <li>
        <a class="page-link" href="/genai-utoronto/assignments/">
            <i class="fas fa-book"></i> Assignments
        </a>
    </li>
    
    <li>
        <a class="page-link" href="/genai-utoronto/project/">
            <i class="fas fa-user-graduate"></i> Project
        </a>
    </li>
    
</ul>

     </div>  
    </nav>

  </div>

  <!-- <div class="header-texture" style="height:100%; z-index: 0; position: absolute; top:0; right: 0; left: 0; 
  background-image: url('/genai-utoronto/_images/pattern.png');" /> -->

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Materials</h1>
  </header>

  <article class="post-content">
    <h2 id="lecture-notes">Lecture Notes</h2>
<p>The lecture notes are uploaded through the semester. For each chapter, the notes are provided section by section.</p>
<h3 id="chapter-0-course-overview-and-logistics">Chapter 0: Course Overview and Logistics</h3>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH0/CH0.pdf">Handouts</a>: All Sections included in a single file</li>
</ul>

<h3 id="chapter-1-text-generation-via-language-models">Chapter 1: Text Generation via Language Models</h3>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH1/CH1_Sec1.pdf">Section 1</a>: Fundamentals of Language Modeling - Primary LMs</li>
  <li><a href="/genai-utoronto/assets/Notes/CH1/CH1_Sec2.pdf">Section 2</a>: Transform-based Models</li>
</ul>

<h2 id="book">Book</h2>

<p>There is indeed no single textbook for this course, and we use various resources in the course. Most of resources are research papers, which are included in the reading list below and completed through the semester. The following textbooks have however covered some key notions and related topics.</p>

<ul>
  <li><a href="https://www.bishopbook.com/">[BB] Bishop, Christopher M., and Hugh Bishop. <em>Deep Learning: Foundations and Concepts.</em> Springer Nature, 2023.</a></li>
  <li><a href="https://probml.github.io/pml-book/book2.html">[M] Murphy, Kevin P. <em>Probabilistic Machine Learning: Advanced Topics.</em> MIT Press, 2023.</a></li>
  <li><a href="https://www.deeplearningbook.org/">[GYC] Goodfellow, Ian, et al. <em>Deep Learning.</em> MIT Press, 2016.</a></li>
</ul>

<p>With respect to the first part of the course, the following book provides some good read:</p>

<ul>
  <li><a href="https://web.stanford.edu/~jurafsky/slp3/">[JM]Jurafsky, Dan, and James H. Martin. <em>Speech and Language Processing.</em> 3rd Edition, 2024.</a></li>
</ul>

<p>The following recent textbooks are also good resources for <strong>practicing hands-on skills.</strong> Note that we are <strong>not</strong> simply learning to implement only! We study the fundamentals that led to development of this framework, nowadays known as <strong>generative AI.</strong> Of course, we try to get our hands dirty as well and learn how to do implementation.</p>

<ul>
  <li><a href="https://www.oreilly.com/library/view/hands-on-generative-ai/9781098149239/">Sanseviero, Omar, et al. <em>Hands-On Generative AI with Transformers and Diffusion Models.</em> O’Reilly Media, Inc., 2024.</a></li>
  <li><a href="https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/">Alammar, Jay, and Maarten Grootendorst. <em>Hands-on large language models: language understanding and generation.</em> O’Reilly Media, Inc., 2024.</a></li>
</ul>

<h2 id="reading-list">Reading List</h2>

<p>This section will be completed gradually through the semseter. I will try to break down the essence of each item, so that you could go over them easily.</p>

<h3 id="review">Review</h3>
<p>You may review the idea of Seq2Seq learning in the following references:</p>
<ul>
  <li><a href="https://pdfs.semanticscholar.org/bba8/a2c9b9121e7c78e91ea2a68630e77c0ad20f.pdf">SimpleLM</a>: Initial ideas on making a language model</li>
  <li><a href="https://arxiv.org/abs/1308.0850">SeqGen</a>: Sequence generation via RNNs –<em>Old idea, but yet worth thinking about it!</em></li>
  <li><a href="https://arxiv.org/abs/1409.3215v3">Seq2Seq</a>: How we can do sequence to sequence learning via NNs</li>
</ul>

<p>You may review the idea of transformers in the following resources:</p>
<ul>
  <li><a href="https://arxiv.org/abs/1706.03762">Transformer Paper</a>: Paper <strong>Attention Is All You Need!</strong> published in 2017 that made a great turn in sequence processing</li>
  <li><a href="https://web.stanford.edu/~jurafsky/slp3/9.pdf">Transformers</a>: Chapter 9 of <a href="https://web.stanford.edu/~jurafsky/slp3/">[JM]</a></li>
  <li><a href="https://www.bishopbook.com/">Transformers</a>: Chapter 12 of <a href="https://www.bishopbook.com/">[BB]</a> <strong>Section 12.1</strong></li>
</ul>

<h3 id="chapter-1-text-generation-and-language-models">Chapter 1: Text Generation and Language Models</h3>
<h4 id="tokenization-and-embedding">Tokenization and Embedding</h4>
<ul>
  <li><a href="https://web.stanford.edu/~jurafsky/slp3/2.pdf">Tokenization</a>: Chapter 2 of <a href="https://web.stanford.edu/~jurafsky/slp3/">[JM]</a></li>
  <li>
    <p><a href="https://web.stanford.edu/~jurafsky/slp3/6.pdf">Embedding</a>: Chapter 6 of <a href="https://web.stanford.edu/~jurafsky/slp3/">[JM]</a></p>
  </li>
  <li><a href="http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM">Original BPE Algorithm</a>: Original BPE Algorithm proposed by Philip Gage in 1994</li>
  <li><a href="https://arxiv.org/abs/1508.07909">BPE for Tokenization</a>: Paper <em>Neural machine translation of rare words with subword units</em> by <em>Rico Sennrich, Barry Haddow, and Alexandra Birch</em> presented in ACL 2016 that adapted BPE for NLP</li>
  <li><a href="https://arxiv.org/abs/1808.06226">SentencePiece</a>: Paper <em>SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing</em> by <em>Taku Kudo and John Richardson</em> presented in EMNLP 2018 that introduces a language-independent tokenizer</li>
  <li><a href="https://arxiv.org/abs/1609.08144">WordPiece</a>: Paper <em>Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</em> by <em>Yonghui Wu et al.</em> published in 2016 introducing WordPiece (used in BERT)</li>
  <li><a href="https://arxiv.org/abs/2105.13626">ByT5</a>: Paper <em>ByT5: Towards a token-free future with pre-trained byte-to-byte models</em> by <em>Xue et al.</em> presented in ACL 2022 proposing ByT5</li>
</ul>

<h4 id="language-modelling">Language Modelling</h4>
<ul>
  <li><a href="https://www.bishopbook.com/">LMs</a>: Chapter 12 of <a href="https://www.bishopbook.com/">[BB]</a> <strong>Section 12.2</strong></li>
  <li><a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf">N-Gram LMs</a>: Chapter 3 of <em>Speech and Language Processing;</em> <strong>Section 3.1</strong> on N-gram LM</li>
  <li><a href="https://www.bishopbook.com/">Maximum Likelihood</a>: Chapter 2 of <a href="https://www.bishopbook.com/">[BB]</a> <strong>Sections 12.1 – 12.3</strong></li>
</ul>

<h4 id="recurrent-lms">Recurrent LMs</h4>
<ul>
  <li><a href="https://web.stanford.edu/~jurafsky/slp3/8.pdf">Recurrent LMs</a>: Chapter 8 of <a href="https://web.stanford.edu/~jurafsky/slp3/">[JM]</a></li>
  <li><a href="https://arxiv.org/abs/1708.02182">LSTM LMs</a>: Paper <em>Regularizing and Optimizing LSTM Language Models</em> by <em>Stephen Merity, Nitish Shirish Keskar, and Richard Socher</em> published in ICLR 2018 enabling LSTMs to perform strongly on word-level language modeling</li>
  <li><a href="https://arxiv.org/abs/1711.03953">High-Rank Recurrent LMs</a>: Paper <em>Breaking the Softmax Bottleneck: A High-Rank RNN Language Model</em> by <em>Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen</em> presented at ICLR 2018 proposing Mixture of Softmaxes (MoS) and achieving state-of-the-art results at the time</li>
</ul>

<h4 id="transformer-based-lms">Transformer-based LMs</h4>
<ul>
  <li><a href="https://www.bishopbook.com/">Transformer LMs</a>: Chapter 12 of <a href="https://www.bishopbook.com/">[BB]</a> <strong>Section 12.3</strong></li>
  <li><a href="https://web.stanford.edu/~jurafsky/slp3/10.pdf">LLMs via Transformers</a>: Chapter 10 of <a href="https://web.stanford.edu/~jurafsky/slp3/">[JM]</a></li>
</ul>

<h4 id="llms">LLMs</h4>
<ul>
  <li><a href="https://arxiv.org/abs/1810.04805">BERT</a>: Paper <em>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em> by <em>Jacob Devlin et al.</em> presented at NAACL 2019 that introduced BERT
<!-- * [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237): Paper _XLNet: Generalized Autoregressive Pretraining for Language Understanding_ by _Zhilin Yang et al._ presented at NeurIPS 2019 that introduces XLNet --></li>
  <li><a href="https://arxiv.org/abs/1907.11692">RoBERTa</a>: Paper <em>RoBERTa: A Robustly Optimized BERT Pretraining Approach</em> by <em>Yinhan Liu, et al.</em> (Facebook AI, 2019) that shows BERT’s performance can be significantly improved by more data, longer training, and removing next sentence prediction</li>
  <li>
    <p><a href="https://arxiv.org/abs/1910.10683">T5</a>: Paper <em>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</em> by <em>Colin Raffel et al.</em> (JMLR 2020) that reformulates all NLP tasks as text-to-text problems introducing the T5 model</p>
  </li>
  <li><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">GPT-1</a>: Paper <em>Improving Language Understanding by Generative Pre-Training</em> by <em>Alec Radford et al.</em> (OpenAI, 2018) that introduced GPT-1 and revived the idea of pretraining transformers as LMs followed by supervised fine-tuning</li>
  <li><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a>: Paper <em>Language Models are Unsupervised Multitask Learners</em> by <em>Alec Radford et al.</em> (OpenAI, 2019) that introduces GPT-2 with 1.5B parameter trained on web text</li>
  <li><a href="https://arxiv.org/abs/2005.14165">GPT-3</a>: Paper <em>Language Models are Few-Shot Learners</em> by <em>Tom B. Brown et al.</em> (OpenAI, 2020) that introduces GPT-3, a 175B-parameter transformer LM</li>
  <li><a href="https://arxiv.org/abs/2303.08774">GPT-4</a>: <em>GPT-4 Technical Report</em> by <em>OpenAI</em> (2023) that provides an overview of GPT-4’s capabilities</li>
</ul>


  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

<!--     <h2 class="footer-heading">University of Toronto</h2> -->
         <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
 

         <p class="text">
The Edward S. Rogers Sr. Department of Electrical and Computer Engineering<br />
University of Toronto<br />

      </div>

      <div class="footer-col  footer-col-2">
       <ul class="social-media-list">
     

          

          
  <li>
    <a href="https://www.bereyhi.com">
      <i class="fas fa-globe" style="color:gray"></i> bereyhi.com
    </a>
  </li>


          

          

          
  <li>
    <a href="https://www.ece.utoronto.ca">
      <i class="fas fa-globe" style="color:gray"></i> ece.utoronto.ca
    </a>
  </li>




       
        </ul>
      </div>
    </div>

  </div>

</footer>

  </body>

</html>
<!-- d.s.m.s.050600.062508.030515.080516.030818 | "Baby, I'm Yours" -->